import os
import json
import re
import time
from chromadb import PersistentClient
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
load_dotenv()

# Constants
HF_TOKEN = os.getenv("TOKEN")
MODEL_NAME = os.getenv("MODEL_NAME")
FIELD_ORDER = ["Skills", "Education", "Experience", "Job Role"]

# Initialize clients
llm_client = InferenceClient(model=MODEL_NAME, token=HF_TOKEN)

# Prompt Templates
system_prompt = """
You are a world-class HR, Talent Acquisition, and Generative AI Specialist with deep expertise in job-role alignment, semantic document comparison, and hiring decision automation.

You are tasked with comparing a candidate resume and a job description. Both are pre-parsed into structured fields: Skills, Education, Job Role, Experience, and Other Information. Your job is to assess the alignment **strictly based on meaning** — not exact keyword matches.

You must return a single valid JSON object in the structure described below.

### Instructions:

- Evaluate **semantic relevance**, not just keyword overlap. For example, treat "ML Engineer" and "Machine Learning Engineer" as identical.
- Use **real-world hiring logic**: If a resume **exceeds** the JD requirements (e.g., more skills, more education, deeper experience), the match_pct should be high — even **100%**.
- Avoid over-penalizing minor differences. Focus on **capability and fit**.
- NEVER hallucinate or infer information not explicitly present in either document.
- NEVER nest objects inside any field — your output must remain a **flat JSON**.
- Explanations must be **insightful, human-readable, and professional** — written as if speaking to a hiring manager.
- Escape any invalid characters like tabs/newlines using `\\t` and `\\n`.
- Do **not** include any commentary or text outside the JSON.

### Field Matching Logic:

1. **Skills**
   - Match based on technical equivalence.
   - If the resume includes **all** required skills or **more**, assign **100%**.
   - If semantically similar (e.g., "pandas" vs. "data manipulation in Python"), still assign high match_pct (80–95%).

2. **Education**
   - If the candidate’s education level is **equal or higher** than the JD, score high.
   - Degrees in relevant fields or from reputable institutions should be favored.

3. **Experience**
   - Match on role relevance, technologies used, domain familiarity, and years of experience.
   - Experience that directly meets or exceeds JD expectations should score high (90–100%).

4. **Job Role**
   - Normalize semantically equivalent titles.
   - Penalize only when there is **significant deviation** in domain, seniority, or functional focus.
   Normalize job titles by semantic equivalence, recognizing hierarchical and domain relationships (e.g., "Machine Learning Engineer" matches "Data Scientist" since ML is part of Data Science). Penalize mismatches only if there is significant deviation in domain, seniority, or functional focus (e.g., engineer vs. manager). Prioritize domain relevance and seniority alignment. Explain penalties when applied.

5. **OverallMatchPercentage**
   - Must be a weighted score calculated **heavily** from Skills, Experience, Education, and Job Role.
   - **Other Information** may provide a minor bonus/penalty (±5%).

6. **AI_Generated_Estimate_Percentage**
   - Estimate how likely the resume was generated by AI, based on repetition, unnatural tone, excessive perfection, or generic phrasing.

### Output Format (strict):

{
  "{resume_filename}": {
    "Skills": {
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    },
    "Education": {
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    },
    "Job Role": {
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    },
    "Experience": {
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    },
    "OverallMatchPercentage": float,
    "why_overall_match_is_this": string,
    "AI_Generated_Estimate_Percentage": float
  }
}

Return ONLY the JSON object. No extra comments or explanation.
"""

user_prompt_template = """
Compare the following resume and job description using their parsed field data.

Each field below is populated from the database. Compare them **semantically and intelligently** using the structure below.

Use the following strict JSON format:

{{
  "{resume_filename}": {{
    "Skills": {{
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    }},
    "Education": {{
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    }},
    "Job Role": {{
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    }},
    "Experience": {{
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    }},
    "OverallMatchPercentage": float,
    "why_overall_match_is_this": string,
    "AI_Generated_Estimate_Percentage": float
  }}
}}

Return only the JSON object, and ensure:
- match_pct values reflect real semantic similarity (not keyword count).
- Explanations are professional, specific, and insightful.
- No nested JSON objects inside any value fields.
- No semicolons (;) in values — use periods or commas.
- No hallucinated info or missing keys.
"""

def get_collection_docs(client, collection_name):
    try:
        collection = client.get_collection(collection_name)
        results = collection.get(include=["documents"])
        docs = results.get("documents", [])
        if docs and isinstance(docs[0], list):
            docs = [d for sublist in docs for d in sublist]
        return docs
    except Exception as e:
        print(f"[ERROR] Failed to load collection '{collection_name}': {e}")
        return []

def build_field_texts(field_names, docs):
    lines = []
    for name, doc in zip(field_names, docs):
        doc_clean = re.sub(rf"^{re.escape(name)}:\s*", "", doc, flags=re.IGNORECASE)
        lines.append(f"{name}: {doc_clean}")
    return "\n".join(lines)

def clean_llm_json(raw_response):
    raw = raw_response.strip()
    raw = re.sub(r"^```(?:json)?|```$", "", raw, flags=re.MULTILINE).strip()
    brace_stack = []
    start_idx = None
    for i, char in enumerate(raw):
        if char == '{':
            if start_idx is None:
                start_idx = i
            brace_stack.append('{')
        elif char == '}':
            if brace_stack:
                brace_stack.pop()
                if not brace_stack:
                    return raw[start_idx:i+1].strip()
    return raw

def normalize_llm_response(data):
    for field in ["Skills", "Education", "Job Role", "Experience"]:
        if field in data:
            for key in ["resume_value", "job_description_value"]:
                value = data[field].get(key)
                if isinstance(value, list):
                    data[field][key] = ", ".join(map(str, value))
    return data

def query_llm(system_prompt, user_prompt, retries=2):
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    for attempt in range(retries):
        try:
            response = llm_client.chat_completion(messages=messages, max_tokens=2048, temperature=0.2)
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"[ERROR] LLM call failed (attempt {attempt+1}): {e}")
            time.sleep(1)
    return ""

def main(resume_db_path, jd_db_path):
    try:
        start_time = time.time()
        
        jd_client = PersistentClient(path=jd_db_path)
        resume_client = PersistentClient(path=resume_db_path)

        # Get all collections (each collection represents one JD or resume)
        jd_collections = [c.name for c in jd_client.list_collections()]
        resume_collections = [c.name for c in resume_client.list_collections()]

        if not jd_collections or not resume_collections:
            raise ValueError("No collections found in the provided database paths")

        all_results = []

        for jd_collection in jd_collections:
            jd_docs = get_collection_docs(jd_client, jd_collection)
            if len(jd_docs) < 5:
                continue

            jd_text = build_field_texts(FIELD_ORDER, jd_docs[:4])
            jd_other_info = jd_docs[4] if len(jd_docs) > 4 else ""

            for resume_collection in resume_collections:
                resume_docs = get_collection_docs(resume_client, resume_collection)
                if len(resume_docs) < 5:
                    continue

                resume_text = build_field_texts(FIELD_ORDER, resume_docs[:4])
                resume_other_info = resume_docs[4] if len(resume_docs) > 4 else ""

                comparison_name = f"{resume_collection}_vs_{jd_collection}"

                user_prompt = user_prompt_template.format(resume_filename=comparison_name)
                user_prompt += f"\n\nJob Description Other Information:\n{jd_other_info}"
                user_prompt += f"\nResume Other Information:\n{resume_other_info}"
                user_prompt += f"\n\nJob Description:\n{jd_text}\n\nResume:\n{resume_text}"

                raw = query_llm(system_prompt, user_prompt)
                if not raw:
                    continue

                try:
                    cleaned = clean_llm_json(raw)
                    parsed = json.loads(cleaned)
                    parsed = {k: normalize_llm_response(v) for k, v in parsed.items()}
                    all_results.append(parsed)
                except json.JSONDecodeError as e:
                    print(f"[ERROR] Failed to parse JSON response for {comparison_name}: {e}")
                    continue
                except Exception as e:
                    print(f"[ERROR] Processing response for {comparison_name}: {e}")
                    continue

        if not all_results:
            raise ValueError("No valid comparisons were generated")

        print(f"\n[INFO] Total time: {time.time() - start_time:.2f} sec")
        return all_results

    except Exception as e:
        print(f"[ERROR] In main comparison function: {e}")
        return []